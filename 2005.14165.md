# Language Models are Few-Shot Learners

[arxiv.org/2005.14165](https://arxiv.org/abs/2005.14165)

### Problem / State of the Art

Modern NLP pipelines pre-train on massive text and then fine-tune for each task, which still demands thousands of labeled examples per task. Humans, by contrast, can often learn from a handful of examples or just instructions. The paper asks whether **scaling up** a general language model can close that gap so the same model solves many tasks via prompting alone (zero/one/few-shot), without gradient updates.

### Goal

Train and evaluate an extremely large autoregressive LM—**GPT-3 (175B parameters, ~10× larger than prior non-sparse LMs)**—to test whether pure prompting (no fine-tuning) can reach or approach SOTA across diverse tasks (translation, QA, cloze, reasoning).

### Challenges

- **Few-shot historically underperforms** fine-tuned models; context is limited (nctx = 2048) so only ~10–100 demonstrations fit.
- **Web-scale data quality & contamination:** large crawls are noisy and may leak benchmark text; deduping and overlap checks are required.
- **Bias & fairness:** internet-trained models inherit social biases; preliminary probes show gender skew in occupation associations.

### Key Mechanism (What they built)

- **Architecture:** GPT-2–style decoder-only Transformer with **alternating dense + locally banded sparse attention**; trained **for 300B tokens** over 8 scales up to 175B.
- **Training data (Table 2.2, p.9):** filtered CommonCrawl + WebText2 + Books1/2 + Wikipedia; **non-proportional mixing** to up-weight curated sets (e.g., WebText2 sampled ~3×).
- **Quality controls:** logistic-regression filter and fuzzy dedup; partial removal of benchmark overlaps (Appendix).
- **Evaluation:** strict **zero/one/few-shot** prompting (no gradient updates), with K examples sampled into the context (Figure 2.1–2.4).

### Key Results (evidence)

- **Scaling → in-context gains:** Aggregate accuracy rises with model size; **few-shot improves fastest** (Figure 1.3, p.5).
- **Cloze & commonsense:**
  - **LAMBADA:** 86.4% few-shot (SOTA at the time); ppl = 1.92. (Table 3.2/H).
  - **HellaSwag:** 79.3% few-shot, beating a fine-tuned 1.5B LM though below SOTA multi-task.
  - **StoryCloze:** 87.7% few-shot (K=70).

- **Closed-book QA (Table 3.3):** **TriviaQA** 71.2% few-shot (zero-shot 64.3%), matching/ exceeding fine-tuned baselines; **WebQuestions** 41.5% few-shot; **NQ** 29.9% few-shot.
- **Reading comprehension:** **CoQA 85.0 F1 few-shot**, a few points shy of human/SOTA fine-tuned. (Figure 3.7).
- **SuperGLUE (Table 3.8):** **71.8 avg few-shot** with 32 in-context examples; near SOTA on COPA/ReCoRD; weak on WiC/RTE/CB. (see Figure 3.8, p.20).
- **Human detection of news samples:** With 3-shot conditioning, **human accuracy falls to ~52% for GPT-3 175B** on ~200-word articles; similar result for ~500-word articles (Tables 3.11–3.12, p.26–27).

### Strengths

- **Compelling empirical case for scale:** smooth power-law loss improvements translate into broad task gains without task-specific training.
- **General-purpose interface:** natural-language prompting as a unified API; strong few-shot competencies on tasks requiring reasoning/ adaptation (anagrams, arithmetic, SAT analogies).
- **Care on contamination & bias:** authors proactively analyze overlap and mark affected results; they also probe demographic biases.

### Improvements (what could be better)

- **Pairwise comparison tasks remain weak** (WiC/RTE/CB; ANLI), suggesting limits of in-context patterning for sentence-pair semantics.
- **Residual contamination risk:** a filtering bug left overlaps; retraining wasn’t feasible, so some results carry caveats.
- **Prompt sensitivity & context limit:** results depend on prompt design and 2048-token windows cap K—important practical constraints not deeply ablated.
- **Bias/ethics:** the gender-occupation skew (83% male-leaning) underscores the need for stronger mitigation and evaluations beyond preliminary probes.

### What I Learned / Liked

- Prompt-only **in-context learning scales with model size** and can rival fine-tuned systems on some benchmarks—especially striking on closed-book TriviaQA and COPA/ReCoRD.
- The **news-generation** study (Tables 3.11/3.12) is an unusually concrete, human-facing demonstration of capability and risk.

### Summary Brief of the Paper

The authors train **GPT-3 (175B)** with filtered web/book/wiki data and evaluate it strictly via **zero/one/few-shot prompting**. Scaling yields substantial across-the-board gains; in few-shot mode GPT-3 reaches or nears SOTA on several tasks (e.g., LAMBADA, HellaSwag, StoryCloze; strong **closed-book** TriviaQA), improves reading comprehension to near-human levels on CoQA, and shows rising **in-context learning** with model size (Figure 1.3). Weaknesses persist on sentence-pair semantics (WiC/ANLI) and known concerns remain around **data contamination** and **bias**. A human study shows GPT-3’s news articles are **hard to distinguish from real** (~52% detection), underscoring both capability and potential misuse.
