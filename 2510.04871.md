# Less is More: Recursive Reasoning with Tiny Networks

[arxiv.org/2510.04871](https://arxiv.org/abs/2510.04871)

### Problem / State of the Art

- Many hard, structured reasoning tasks (Sudoku, mazes, ARC-AGI) remain brittle for LLMs even with chain-of-thought (CoT) and heavy test-time compute (TTC). CoT needs high-quality rationales and TTC is expensive; single wrong tokens doom outputs. HRM (Hierarchical Reasoning Model) recently showed strong results by recursing two small nets at different “frequencies” plus deep supervision—but its mechanics and theoretical footing (fixed-point + 1-step gradient) are murky.
- The paper argues we can keep the _good_ (deep supervision + recursion) and drop the _complexity_ (two nets, IFT assumption, extra ACT pass), aiming for a smaller, clearer system. (See intro and Sec. 2–3; the block diagram on p.1.)

### Goal

- Introduce **Tiny Recursive Model (TRM)**—a single, tiny 2-layer network that repeatedly refines a latent “reasoning” state and the current answer; match or beat HRM and many LLMs on puzzle benchmarks with orders of magnitude fewer parameters and simpler training/inference. (Abstract; Fig. 1 on p.1.)

### Challenges

- **Training depth vs. memory:** Recursion implies deep effective computation without full BPTT.
- **Theoretical validity:** HRM’s 1-step gradient assumes near fixed-points that may not be reached in practice.
- **Sample scarcity & overfitting:** Datasets have ~1K training items; bigger nets overfit. (Sec. 2.3, 3.1, 4.4.)

### Key Mechanism (how TRM works)

- **State split:** Keep two features only—`y` = current solution embedding; `z` = latent reasoning. Recursively: update `z ← net(x, y, z)` _n_ times, then update `y ← net(y, z)`. Repeat T times; only the last recursion with gradients, the earlier T−1 are no-grad “improvement passes.” (Alg. on p.5, Fig. 3 pseudocode.)
- **Single tiny network:** One shared 2-layer module handles both roles; the presence/absence of `x` in inputs disambiguates tasks. (Sec. 4.3.)
- **Drop IFT/1-step gradient:** Backprop through the _full_ last recursion instead of assuming fixed-points. (Sec. 4.1.)
- **Simpler ACT:** Predict a halting probability with BCE on “is answer correct?”—no Q-learning “continue” head, so no second forward pass. (Sec. 4.6; contrast HRM’s pseudocode on p.3 with TRM’s on p.5.)
- **Small is better:** Use 2 layers with more recursions; add EMA for stability on tiny data. (Table 1 on p.5; Sec. 4.4, 4.7.)
- **Architecture choice:** Attention-free MLP-Mixer-style works best for short fixed contexts (Sudoku 9×9); attention is better for 30×30 grids (Maze/ARC). (Sec. 4.5.)

### Key Results

- **Ablations (Sudoku-Extreme):** TRM (T=3, n=6, 2-layer single net) hits **87.4%** test acc vs HRM **55.0%**; removing EMA → 79.9%; 1-step gradient → 56.5% (bad); attention version → 74.7%. (Table 1, p.5.)
- **Depth/compute trade:** Matched effective depth, TRM dominates HRM; too many recursions OOM but sweet spot around T=3, n~6. (Table 3, p.8.)
- **Benchmarks:**
  - **Maze-Hard (30×30):** TRM-Att **85.3%** vs HRM **74.5%**.
  - **ARC-AGI-1:** TRM-Att **44.6%** vs HRM **40.3%**.
  - **ARC-AGI-2:** TRM-Att **7.8%** vs HRM **5.0%**.
  - Params: TRM 7M vs HRM 27M. (Tables 4–5 on p.8; figure/table panels show model lists.)

- **Vs LLMs (few-shot/CoT/TTC):** Most large models score lower on ARC-1/2 than 7M-parameter TRM (exceptions exist at the very top with bespoke systems). (Table 5, p.8.)

### Strengths

- **Clarity & simplicity:** Removes contentious IFT fixed-point assumption; single network; simpler ACT. (Sec. 3–4.)
- **Data-efficient generalization:** Recursion + deep supervision + EMA tame overfitting in tiny-data regimes. (Table 1; Sec. 4.7.)
- **Strong empirical evidence:** Careful ablations (layers, attention, ACT, features count) support design choices; the Sudoku example on p.12 visually shows `y` is decodable while `z` is latent.

### Improvements (what could be better)

- **Theory of why recursion helps:** The paper suspects overfitting effects but offers no formal account; scaling laws and general theory would help. (Sec. 6.)
- **Generality beyond grids:** Results are on grid puzzles; assessing text/math/program synthesis or non-grid combinatorics would test robustness.
- **Compute/memory profile:** Backprop through full recursions constrains n due to OOM; a middle-ground strategy (without hurting generalization) would be valuable. (Table 3 notes OOM.)
- **ACT learning signal:** Using correctness as a halting target is pragmatic; exploring uncertainty-aware or value-based halting (without extra pass) might improve training dynamics.
- **ARC training protocol details:** Heavy augmentation is central; more analysis isolating augmentation contribution would clarify data vs. model effects. (Sec. 5 dataset notes.)

### What I Learned / Liked

- The **`y`/`z` reinterpretation** (solution vs. reasoning) demystifies HRM’s “hierarchy” and explains why **exactly two** features are enough—drop one and you conflate solution with reasoning; add more and you fragment state without benefit. (Sec. 4.2; Table 2 on p.6.)
- **Less-is-more** was convincingly demonstrated: 2 layers + more recursion out-generalized deeper nets on tiny data. (Sec. 4.4; Table 1.)

### Summary Brief of the Paper

This work proposes **TRM**, a minimal recursive reasoning model that alternates refining a latent reasoning state `z` and a current answer `y` with a **single 2-layer network**. It drops HRM’s two-network hierarchy, avoids the fixed-point/1-step gradient assumption by **backpropagating through a full final recursion**, and simplifies halting so there’s **no extra forward pass**. With **7M parameters**, TRM achieves **85.3%** on Maze-Hard, **44.6%** on ARC-AGI-1, **7.8%** on ARC-AGI-2, and **87.4%** on Sudoku-Extreme (MLP variant), surpassing HRM’s 27M-param baseline and many LLMs using CoT/TTC. The paper backs its choices with ablations (features count, attention vs. MLP, layers, ACT, EMA) and suggests recursion plus deep supervision mitigate overfitting in small-data regimes. (See Fig. 1 p.1; Tables 1, 3–5 p.5–8; example on p.12.)
