# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

[arxiv.org/1810.04805](https://arxiv.org/abs/1810.04805)

### Problem / State of the Art

Before BERT, transfer learning in NLP split into (a) **feature-based** methods like ELMo (bi-LMs used as frozen features) and (b) **fine-tuning** methods like OpenAI GPT (a left-to-right LM fine-tuned per task). Both rely on **unidirectional** language modeling, which limits how well models can use **both left and right context**, especially harmful in token-level tasks like QA. (See pp. 1–2; Figure 3, p. 13.)

### Goal

Pretrain a **deep bidirectional** Transformer that (1) fuses left/right context at every layer and (2) can be fine-tuned with **minimal task-specific changes** to reach SOTA on many tasks (GLUE, SQuAD, SWAG). (Abstract, p. 1.)

### Challenges

- **Bidirectionality without leakage:** classic LMs can’t be trained bidirectionally because tokens could “see themselves.” (Sec. 3.1, p. 4.)
- **Pretrain–finetune mismatch:** introducing special tokens during pretraining that won’t appear at fine-tune time (e.g., `[MASK]`) risks a mismatch; BERT mitigates it with a mixed masking strategy. (Sec. 3.1; App. A.1.)
- **Compute and sequence length:** attention is quadratic in sequence length; training on long documents is expensive; they stage training (mostly 128 tokens, then 512). (App. A.2.)

### Key Mechanism

- **Architecture:** multi-layer **Transformer encoder** (no decoder), with two published sizes: **BASE** (12×768, 110M) and **LARGE** (24×1024, 340M). (Sec. 3, p. 3.)
- **Inputs:** WordPiece; special tokens `[CLS]` (pooled sequence rep) and `[SEP]`; **segment embeddings** to mark sentence A/B. (Sec. 3; Figure 2 on p. 5 shows token/segment/position embedding sum.)
- **Pretraining tasks:**
  1. **Masked LM (MLM):** mask 15% tokens; of those, 80%→`[MASK]`, 10%→random, 10%→unchanged—predict originals. (Sec. 3.1; App. A.1.)
  2. **Next Sentence Prediction (NSP):** binary task: is B sentence the true next sentence after A? (Sec. 3.1.)

- **Fine-tuning:** same backbone across tasks; add a small head: use `[CLS]` for classification; token reps for tagging/QA. (Figure 1 on p. 3; Figure 4 on p. 15 shows heads for sentence-pair, single-sentence, QA span, and NER.)
- **Data & training:** BooksCorpus + English Wikipedia; 1M steps with large batches; Adam + warmup; predominantly 128-token sequences, then 512 for position learning. (App. A.2–A.3.)

### Key Results

- **GLUE (Table 1, p. 6):** BERTLARGE hits **80.5** on the official GLUE score and strong across tasks (e.g., MNLI **86.7/85.9**), improving substantially over prior SOTA and GPT.
- **SQuAD v1.1 (Table 2, p. 7):** single **BERTLARGE** achieves **90.9 F1** on Dev; with TriviaQA pre-fine-tuning and ensembling, **Test F1 93.2**.
- **SQuAD v2.0 (Table 3, p. 7):** single **BERTLARGE** gets **83.1 F1** Test, +5.1 over previous best.
- **SWAG (Table 4, p. 7):** **86.3%** Test accuracy, +8.3 over GPT baseline.
- **Ablations (Table 5, p. 8):** removing **NSP** hurts QNLI/MNLI/SQuAD; forcing **left-to-right** pretraining is worse across tasks; adding a BiLSTM at fine-tune helps a bit for QA but still trails bidirectional pretraining.
- **Scaling (Table 6, p. 9):** larger models consistently help—even on small datasets like MRPC.
- **Feature-based (Table 7, p. 9):** as frozen features, concatenating top 4 layers is within 0.3 F1 of full fine-tuning on CoNLL-2003 NER.

### Strengths

- **Unified recipe:** one backbone + tiny heads covers sentence- and token-level tasks (Figure 1).
- **Strong empirical lift:** clear, across-the-board SOTA with careful ablations supporting claims.
- **Simple yet impactful pretext tasks:** MLM enables genuine bidirectionality; NSP injects discourse-level pairing.

### Improvements (what could be better)

- **NSP diagnostic depth:** Paper shows NSP helps, but more granular analysis (error types, alternatives like harder negatives or multi-sentence discourse) would clarify _why_.
- **Masking strategy space:** Appendix C.2 probes a few ratios; exploring span masking or dynamic masking (beyond this paper) could test robustness.
- **Data & domain breadth:** Only English Books/Wikipedia; results on diverse domains/languages would test generality.
- **Efficiency reporting:** Training on large TPUs is noted; more cost/performance curves (epochs vs. downstream) would help practitioners plan budgets.

### What I Learned / Liked

- The **80/10/10** masking trick neatly reduces pretrain–finetune mismatch while keeping bidirectional signals strong.
- The **“just add a small head”** fine-tune story (esp. QA start/end vectors) is refreshingly simple yet competitive. (Figure 1, p. 3; SQuAD setup, p. 6.)

### Summary Brief

BERT pretrains a **bidirectional Transformer encoder** using **Masked LM** plus **Next Sentence Prediction**, on BooksCorpus+Wikipedia, and then **fine-tunes** the same network with tiny heads for each task. This design delivers large, consistent **SOTA gains** on GLUE, SQuAD v1.1/v2.0, and SWAG. Ablations show **bidirectionality** and **NSP** matter, and **scaling** helps even in low-resource settings. The method is conceptually simple, broadly applicable, and empirically strong—marking a step-change for transfer learning in NLP. (Abstract; Figures 1–2; Tables 1–7.)
