# Learning to summarize from human feedback

[arxiv.org/2009.01325](https://arxiv.org/abs/2009.01325)

### Problem / State of the Art

Supervised fine-tuning for summarization usually optimizes likelihood of human demos and evaluates with ROUGE—both are weak proxies for what people actually value in a summary. There’s a misalignment between MLE training and human-judged quality, and ROUGE has been criticized for poor correlation with human preferences. The paper frames this as a general misalignment issue and chooses summarization as a concrete testbed.

### Goal

Replace proxy objectives with one that tracks **human preference**: learn a reward model from human comparisons and use it to fine-tune a summarization policy so outputs better match what humans like. The authors aim to show clear quality gains on Reddit TL;DR and transfer to CNN/DM without news-specific fine-tuning.

### Challenges

- **Data & evaluation**: obtaining reliable, scalable human judgments and keeping high agreement; they report labeler–researcher agreement ~77% vs researcher–researcher ~73%.
- **Confounds**: length strongly affects perceived quality, so they control for it (see _Figure 10_ on p. 30), finding the feedback-trained model is still preferred after control.
- **Reward misspecification / over-optimization**: optimizing a learned reward can overfit and diverge from true human preferences (_Figure 5_ on p. 8).
- **Compute & cost**: fine-tuning the 6.7B model with RL took ~320 GPU-days.
- **Dataset quirks**: on CNN/DM, “lead-3” often beats the reference highlights, which complicates evaluation baselines.

### Key Mechanism (How it works)

1. **Collect pairwise comparisons**: for a Reddit post, humans pick the better of two summaries sampled from multiple policies/baselines (_Figure 2_, p. 4).
2. **Train a reward model (RM)** with a Bradley–Terry style loss over pairs:
   $\text{loss} = -\log \sigma\big(r_\theta(x,y_i)-r_\theta(x,y_{1-i})\big).$
3. **Optimize the policy with PPO**, using the RM output as the episodic reward and adding a **KL penalty** to keep the RL policy near the supervised model:
   $R(x,y)=r_\theta(x,y)-\beta\log\big[\pi_{\text{RL}}(y|x)/\pi_{\text{SFT}}(y|x)\big]$. They also use a separate value network. (_Section 3.4_; _Figure 2_).
   Design choices include filtering TL;DR to 24–48 token references to reduce length confounds, and treating the summary as a single episode with $\gamma=1$.

### Key Results

- **TL;DR quality (Figure 1, p. 2)**: The 1.3B RLHF model beats a supervised model ~10× larger (61% vs 43% preference over references); the 6.7B RLHF model does even better. After controlling for length, the 6.7B model is still preferred ~65% of the time.
- **Axis-wise quality (Figure 3, p. 7)**: Humans rate RLHF higher than supervised on coverage, coherence, accuracy, and overall; 6.7B PPO summaries get a perfect 7/7 **45%** of the time vs 20–23% for supervised/reference.
- **Zero-shot transfer to CNN/DM (Figure 4, p. 7)**: The 6.7B TL;DR-trained RLHF model nearly matches a T5 model fine-tuned on CNN/DM (at similar lengths), despite no news-specific fine-tuning.
- **Better metric than ROUGE**: Their RM predicts human preferences better than ROUGE and optimizing RM improves human-judged quality, whereas optimizing ROUGE peaks early and lower (Figure 7).
- **Less copying**: Bigram-overlap analysis (Table 16, p. 34) shows the 6.7B RLHF model copies less than supervised/pretrained baselines on both TL;DR and CNN/DM.
- **Scaling laws for RM (Figure 6, p. 8)**: Doubling data adds ~1.1% RM validation accuracy; doubling model size adds ~1.8%; the 6.7B RM approaches single-human accuracy.

### Strengths

- **Clear, general recipe** (collect comparisons → learn RM → PPO with KL control) with strong QC and agreement checks.
- **Thorough analysis**: length controls, transfer, scaling, and reward-overfitting diagnostics (Figures 4–7).
- **Open data**: 64,832 TL;DR comparisons released for the community.

### Improvements

- **Baselines with matched labeling budgets**: the authors note they didn’t collect a similarly expensive set of demonstrations to create a fully matched supervised baseline, which limits some comparisons.
- **Robustness of learned rewards**: Over-optimization steadily harms true preference, so more regularizers, off-policy evaluation, or adversarial audits of the RM could help (_Figure 5_).
- **Generalization breadth**: Results are strong on TL;DR and transfer to CNN/DM; evaluation on other domains/languages and longer documents would strengthen external validity (they highlight CNN/DM reference fragility).
- **Cost**: RL fine-tuning is expensive (~320 GPU-days); exploring cheaper preference-learning (e.g., rejection sampling / DPO-style objectives) could reduce compute burden.

### What I Learned / Liked

- Pairwise human comparisons scale better than point-wise Likert supervision for **training** the RM, and the KL-regularized PPO objective is a practical way to steer models toward human preferences without drifting off-distribution. The visual diagnostics—_Figure 5_ for reward over-optimization and _Figure 6_ for RM scaling—make the lessons very tangible.

### Summary Brief

The paper replaces proxy training/evaluation for summarization with **human preference optimization**: collect comparisons, train a **reward model**, and fine-tune with **PPO + KL**. On Reddit TL;DR, RLHF models beat much larger supervised baselines and are preferred to reference summaries; after controlling for length, the 6.7B model’s outputs are still preferred ~65% of the time. The TL;DR-trained model **transfers** to CNN/DM and nearly matches a news-fine-tuned T5 at similar lengths. Their reward model **predicts human preferences** better than ROUGE, and optimizing ROUGE can **hurt** human-judged quality. They analyze **over-optimization** of learned rewards and provide **scaling** observations for RM size and data. The work is a foundational demonstration that aligning language models with **human preferences** can be done at scale and yields better summaries than optimizing proxies.
