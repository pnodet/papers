# Executable Code Actions Elicit Better LLM Agents

[arxiv.org/2402.01030](https://arxiv.org/abs/2402.01030)

### Problem / State of the Art

Tool-using LLM agents usually express actions as **text or JSON** with fixed schemas; that constrains what they can do and makes composing multiple tools awkward. Prior “code-generation for problem solving” often emits a one-shot program and can’t adapt based on new observations. The paper asks how to **expand and standardize the action space** so agents can solve more complex, real-world tasks.

### Goal

Introduce **CodeAct**: treat _executable Python code_ as the agent’s action space; show it’s a stronger general tool-use interface than text/JSON; then build an open-source agent, **CodeActAgent**, trained on **CodeActInstruct** (≈7k multi-turn trajectories) to realize these benefits without hurting general LLM skills.

### Challenges

- Fixed-format actions limit **control/data flow** and multi-tool composition.
- Single-turn code approaches can’t **revise actions** from feedback.
- Open-source models lag closed-source on complex tasks under zero-shot settings.

### Key Mechanism (How it works)

- **Unified action = Python code** executed in a sandbox; observations include results and error messages. This natively supports loops, branching, variable reuse (control/data flow), and self-debugging.
- Leverages existing **software packages** (Pandas, scikit-learn, Matplotlib, etc.) rather than curating bespoke tools.
- Empirically motivated by **Table 1** (benefits vs text/JSON: data availability, complex ops, tool availability, automated feedback).

### Key Results

**RQ1 – Atomic tool calls (API-Bank):** CodeAct is comparable or better than text/JSON for most LLMs; for open-source models, it’s often the best format, while closed-source models sometimes favor JSON (likely due to targeted tuning). (Table 2)

**RQ2 – Multi-tool, multi-turn (M3ToolEval, 82 tasks):** CodeAct yields **higher success** and **fewer interaction turns** across many models. For example, **gpt-4-1106-preview**: _74.4%_ with CodeAct vs _53.7% text_ / _52.4% JSON_, using **2.1 fewer turns** on average. (Table 3; Fig. 1 bottom)

**Agent training:** The authors collect **CodeActInstruct** (≈7k multi-turn interactions built from repurposed datasets) and fine-tune **LLaMA-2-7B** and **Mistral-7B** into **CodeActAgent**. On **MINT** and **M3ToolEval**, CodeActAgent (Mistral-7B) beats other open-source 7B/13B models and approaches some 70B results; it also **maintains/improves** generic LLM benchmarks. (Table 4/5)

### Strengths

- **Clear, unified interface** that inherits programming affordances (loops, variables), enabling complex tool composition and self-debug.
- **Practicality:** direct use of Python ecosystem; less bespoke tooling.
- **Evidence across 17 LLMs**; quantitative gains include up to **+20% absolute success** and **~30% fewer actions** on complex tasks.
- **Open-source pipeline:** data mixture (CodeActInstruct) + models (CodeActAgent).

### Improvements (constructive)

- **Benchmark scale & diversity:** M3ToolEval has **82** curated tasks—useful but small; scaling domains and tools would strengthen claims.
- **Training data provenance:** Most trajectories come from GPT-3.5/Claude (6,728) with only 411 from GPT-4; more human / harder trajectories could reduce distillation bias.
- **Security & safety:** Executable-code agents raise risks; the paper flags possible sandbox escape/cyber-attack scenarios but offers limited mitigations—future work should formalize **permissions, auditing, and isolation**.
- **Format trade-offs:** For some closed-source LLMs, **JSON** is still best in atomic calls; exploring hybrid interfaces or adapter prompts could help.

### What I Learned / Liked

A simple reframing—**“code as the action language”**—unlocks control/data flow, native feedback, and tool reuse. The paper also shows how to **bootstrap an agent** from synthetic multi-turn trajectories while preserving general abilities.

### Summary Brief

The paper proposes **CodeAct**, a framework where agents emit **executable Python** instead of text/JSON actions. Thanks to programming primitives and the Python ecosystem, agents can compose tools, reuse intermediate results, and self-debug. Across atomic (API-Bank) and complex (M3ToolEval) settings, CodeAct consistently matches or beats alternatives; with **CodeActInstruct**, the fine-tuned **CodeActAgent** further improves open-source agents without sacrificing general skills. The idea is elegant, practical, and empirically supported—though scaling benchmarks, broadening data, and hardening safety will be key next steps.
