# A Proof of Learning Rate Transfer under μP

[arxiv.org/2511.01734](https://arxiv.org/abs/2511.01734)

### Problem / State of the Art

Tuning learning rate (LR) typically depends on width, which makes scaling models expensive. µP (Maximal Update Parametrization) was proposed to stabilize hyperparameters at large width, but until now there wasn’t a rigorous proof that the optimal LR actually “transfers” (converges to a width-independent constant).

### Goal

Prove, for deep **linear** MLPs trained with GD and parametrized with µP, that the **optimal** LR converges to a **non-zero** constant as width → ∞ (learning-rate transfer), characterize the convergence rate at one step, and show the phenomenon fails under SP/NTP.

### Challenges

1. The “optimal LR” is the **argmin of a random loss function** (random init), so one has to prove convergence of an argmin of a stochastic process.
2. Beyond the first step, the network output becomes a **polynomial in $\eta$ with growing degree**, so controlling higher-order monomials and their width-limits is nontrivial.

### Key Mechanism (How they do it)

- **Set-up & definition.** They formalize “LR transfer” as: the optimal LR $\eta_n^{(t)}$ converges (in probability) to some deterministic $\eta_\infty^{(t)} > 0$ as width $n\to\infty$.
- **Polynomial-in-$\eta$ view.** Show the t-step model output $f^{(t)}(x)$ is a **polynomial in $\eta$** whose coefficients (determined by initialization) converge almost surely to deterministic limits via Tensor Programs. Hence, the t-step loss $L_n^{(t)}(\eta)$ converges uniformly on compacts to a deterministic polynomial $L_\infty^{(t)}(\eta)$.
- **Step t = 1 in detail.** For one step, coefficients of degree ≥2 shrink with width; the degree-1 term converges to a **non-zero** constant. The loss becomes “quasi-quadratic” in $\eta$, enabling an explicit limit and a rate for the optimizer.
- **Argmin stability.** With the limiting polynomial well-behaved (positive leading term), the argmin of L*n^(t) converges to that of L*∞^(t). The proof uses uniform convergence and standard argmin/derivative arguments; almost-sure convergence is obtained via Tensor Programs.

### Key Results

- **Theorem 1 (t=1):** The optimal LR under µP converges to a **strictly positive constant** $\eta_\infty^{(1)}$ with rate **$O(n^{-1/2})$** on compact $\eta$-intervals.
  *Empirics:* On synthetic linear data, measured optima across widths $2^k$ concentrate around the theoretical limit; the observed rate matches $n^{-1/2}$ up to ~1024 width before slowing (Fig. 2, page 7).
- **Failure under SP/NTP:** Under **SP**, $\eta_n^{(1)} \to 0$ as $n\to\infty$, so LR does **not** transfer; under **NTP**, the effective LR scales down with $n^{-1/2}$ and the optimal LR blows up—again, no transfer.
- **General t (Theorem 3):** For any step t, $f^{(t)}(x)$ remains a polynomial in $\eta$ with coefficients converging almost surely; with a mild uniqueness assumption, the optimal LR converges almost surely to a **non-zero** constant, i.e., LR transfer holds at **any finite training step**.
  _Empirics:_ Fig. 3 (page 10) shows consistent optima at t=5 and t=10 with µP, while SP’s optimum keeps shifting; Fig. 4–5 show similar behavior across depths and even with **ReLU + Adam**, hinting at broader validity (though not proved).

### Strengths

- **First rigorous proof** of LR transfer under µP for deep linear nets, plus an explicit **O(n^{-1/2})** rate at one step.
- **Clean mechanism:** The polynomial-in-$\eta$ lens makes the phenomenon intuitive and tractable; Tensor Programs delivers almost-sure convergence of coefficients.
- **Good diagnostics:** Clear contrast with SP/NTP isolates **why** µP transfers—head scaling and feature-learning vs. kernel/lazy limits.

### Improvements

- **Scope:** Results are for **linear** MLPs with **GD**; the paper offers empirical hints for ReLU/Adam but no theory yet. Extending to nonlinear nets/optimizers is left open.
- **Assumptions:** Requires Ky≠0 and (for general t) uniqueness of the limiting minimizer; also the training keeps W₀ and V **fixed**, which simplifies analysis. It’d be useful to quantify how conclusions change when training all layers/heads.
- **Tightness:** Empirically the n^{-1/2} rate looks loose at large widths; a sharper finite-width theory would help practitioners. (See Fig. 2 discussion.)

### What I Learned / Liked

- Viewing training **step-t** behavior as a **deterministic polynomial limit in $\eta$** is a simple, powerful abstraction for hyperparameter transfer proofs.
- The SP vs µP contrast isolates the effect of **head variance scaling** (n^{-1} vs n^{-2}) on LR behavior—great intuition for why defaults fail to transfer.

### Summary Brief

The paper proves that for deep **linear** MLPs trained with GD under **µP**, the **optimal learning rate** converges to a **non-zero constant** as width grows; at **t=1** it converges at **$O(n^{-1/2})$**. Under **SP**, the optimal LR → 0; under **NTP**, it diverges—so no transfer there. The core technique is to express t-step outputs as **polynomials in $\eta$** and show their coefficients **converge** almost surely via Tensor Programs, which stabilizes the argmin. Experiments back the theory, and suggest similar transfer for **ReLU + Adam**, though that extension remains open.
