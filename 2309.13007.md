# RECONCILE: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs

[arxiv.org/2309.13007](https://arxiv.org/abs/2309.13007)

### Problem / State of the Art

LLMs still stumble on multi-step reasoning. Self-reflection and single-model multi-agent debate help, but they often recycle the same blind spots because all agents share one backbone, data, and inductive biases. That creates echo chambers, poor calibration, and limited diversity of ideas. RECONCILE asks: if we make the agents _different models_ and teach them to discuss, persuade, and vote with calibrated confidence, can we reach better consensus answers?

### Goal

Design a **multi-model, multi-agent** framework that (1) elicits diverse lines of reasoning, (2) lets agents **convince** one another using demonstration-based “corrective” explanations, and (3) aggregates with **confidence-weighted** voting to improve both team and individual performance on reasoning tasks. (See the overview diagram on p.2 and workflow on p.4.)

### Challenges

- **Lack of diversity** in prior multi-agent setups (same model cloned) → correlated errors.
- **Confidence overconfidence** in LLMs → naive majority or self-reported confidence can mislead.
- **Discussion quality** → agents need examples of _how_ to persuade peers toward the gold answer.
- **Cost/latency** → multi-round discussions multiply API calls, so convergence speed matters. (Their Fig. 4 studies rounds vs. consensus.)

### Key Mechanism

RECONCILE runs a “round-table conference” in **three phases** (Fig. 2, p.4):

1. **Initial generation**: each agent (e.g., ChatGPT, Bard, Claude2) produces an answer, CoT explanation, and a **self-estimated confidence**.
2. **Multi-round discussion**: in each round, every agent receives a **discussion prompt** containing (a) **grouped** answers & explanations from all agents, (b) their **confidences**, and (c) a few **convincing samples**—short human explanations that previously flipped a wrong answer for a _specific_ target model. Agents revise answer, explanation, and confidence. (Figure 3 details how convincing samples are selected per agent.)
3. **Team answer**: compute a **calibrated confidence-weighted vote** using a simple bucket mapping (e.g., 1.0/0.8/0.5/0.3/0.1) to reduce overconfidence effects; terminate when consensus reached or rounds exhausted. (Recalibration and ECE reduction shown on pp.16 & 18.)

### Key Results

Across seven benchmarks (commonsense, math, logic, NLI), **RECONCILE beats strong single-agent and single-model multi-agent baselines** (Table 2, p.7):

- **StrategyQA**: 79.0% vs best debate baseline 71.3% (+7.7).
- **Date Understanding**: 86.7% vs best baseline 75.3% (+11.4).
- **CSQA**: 74.7% (on par or better than GPT-4 zero-shot in their setup).
- **Math**: modest gains with general models; but with a domain expert agent (DeepSeekMath), **MATH** rises to **58.3%**, beating best single-agent (50.5) and GPT-4 debate (48.7). (Table 4, p.7.)
- **With GPT-4 or LLaMA-2-70B added**: StrategyQA reaches **87.7%**, **+12.1** over GPT-4 zero-shot (Table 3, p.7).
- **ANLI** (NLI): 57.7% vs debate 48.3% (+9.4).
  They also show:
- **Ablations (Table 7, p.8)**: removing multi-model diversity −6.8 pts; removing convincing samples −4.5; removing confidence estimation −1.3.
- **Diversity metric (Table 5, p.8)**: multi-model agents yield the **lowest similarity** (most diverse) explanations and the best accuracy.
- **Convergence** (Fig. 4, p.9): RECONCILE reaches **full consensus by round 3**; debate leaves ~13% unresolved after 4 rounds.
- **Confidence rescaling** improves calibration (ECE drops; Fig. 9, p.18).
- **Individual agents** improve over rounds, not just the team (Table 8, p.9).

### Strengths

- **Principled use of diversity**: different model families dramatically reduce correlated errors; empirically validated via BERTScore diversity and ablations.
- **Actionable prompts**: grouped opinions + targeted convincing exemplars elevate the _quality_ of discussion beyond simple concatenation or debate.
- **Simple, robust aggregation**: lightweight calibration buckets consistently beat majority vote and max-confidence selection across tasks.
- **Flexible composition**: can plug in API, open-source, or domain-specific models; gains persist and even exceed GPT-4 zero-shot on some sets. (Tables 3–4.)

### Improvements (what could be better)

- **Cost/latency analysis**: while rounds and calls are discussed and some comparisons to self-consistency are provided, a fuller cost-vs-accuracy curve and wall-clock analysis (by model & dataset) would guide practitioners.
- **Confidence modeling**: relies on **post-hoc, self-reported** confidence and bucket heuristics. A learning-based, small-data calibrator or cross-agent calibration could be stronger. (They acknowledge this limitation.)
- **Convincing samples sourcing**: depends on human explanations; where those are missing, they show gains without them, but an automated way to mine/construct convincing exemplars per model would help scalability.
- **Robustness & failure modes**: more analysis on cases where persuasive but wrong rationales sway the team, and safeguards (e.g., verification modules) would be valuable.
- **Full-set coverage**: many main tables use 100-example subsets; they do report full-set checks on two tasks (Table 9) but broader full-set runs would strengthen generality.

### What I Learned / Liked

- The **convincing-sample** idea is clever: demonstrate _how to flip a specific model_ when it’s wrong, then feed those exemplars to its peers during discussion. It makes “debate” practical rather than purely rhetorical.
- **Confidence bucketization** is refreshingly pragmatic—simple, transferable across tasks, and actually improves ECE.
- The **consensus-speed plots** (Fig. 4) are a nice touch; they treat discussion as an algorithm with measurable convergence.

### Summary Brief of the Paper

RECONCILE is a **multi-model round-table** where diverse LLMs iteratively discuss, **persuade** with demonstration-based corrective explanations, estimate confidence, and **vote via calibrated weights**. On seven benchmarks it **outperforms** strong single-agent (Self-Refine, Self-Consistency) and single-model multi-agent debate systems—beating GPT-4 zero-shot on several datasets—and it generalizes to mixes that include GPT-4, LLaMA-2-70B, and DeepSeekMath. Ablations show that **model diversity** and **convincing exemplars** are the biggest contributors, and simple **confidence rescaling** improves both accuracy and calibration. Limitations include reliance on API black boxes, post-hoc confidence, and the need for curated convincing samples, but the framework is modular and practical for real-world ensembles.
