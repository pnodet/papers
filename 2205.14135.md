# FlashAttention: Fast and Memory-Eﬃcient Exact Attention with IO-Awareness

[arxiv.org/2205.14135](https://arxiv.org/abs/2205.14135)

### Problem / State of the Art

Self-attention is quadratic in sequence length for both time and memory, so training/inference bogs down on long contexts. Many “efficient attention” papers cut FLOPs via sparsity or low rank, but often don’t speed up wall-clock time because they ignore GPU memory traffic (HBM↔SRAM). The authors argue the true bottleneck on modern GPUs is IO, not compute. (See the memory-hierarchy diagram and GPT-2 timing in **Fig. 1, p. 2**.)

### Goal

Design an **IO-aware, exact** attention algorithm that:

- avoids materializing the full $N\times N$ matrix on HBM,
- fuses ops into one kernel,
- cuts HBM reads/writes enough to win on **wall-clock**, not just FLOPs,
- and generalizes to block-sparse patterns for even larger contexts.

### Challenges

- **Softmax coupling**: row-wise normalization seems to require global access to scores.
- **Backward pass**: standard implementations store $S=QK^\top$ and $P=\mathrm{softmax}(S)$ ($\Theta(N^2)$ memory) to compute grads.
- **GPU memory hierarchy**: SRAM is fast but tiny; HBM is large but slow—most attention time is memory-bound (arithmetic intensity/roofline). (Sec. 2; **Fig. 2 left, p. 6** shows runtime tracking HBM GB moved.)

### Key Mechanism (what they actually do)

- **Tiling with online softmax**: Split $(Q,K,V)$ into SRAM-sized blocks. For each $(K,V)$ block, stream over $Q$ blocks; maintain per-row running **max** $m$ and **sum** $\ell$ so softmax can be done incrementally and stably. (Alg. 1, Sec. 3.1.)
- **Recomputation instead of storage (backward)**: Store only $O$ plus $(m,\ell)$ and RNG state; during backward, **recompute** local $(S,P)$ on-chip and apply the simplified gradients derived in App. B, avoiding $O(N^2)$ intermediates. (Alg. 4.)
- **Kernel fusion**: One CUDA kernel handles matmul → mask → softmax → dropout → matmul, minimizing HBM traffic. (Sec. 3.1/2.1.)
- **Block-sparse extension**: Skip zero blocks under a fixed block mask (e.g., butterfly); same tiled algorithm, fewer IOs. (Sec. 3.3, Alg. 5.)

### Key Results (evidence)

- **IO complexity**: Standard attention needs $\Theta(Nd+N^2)$ HBM accesses; **FlashAttention** needs $\Theta\big(\tfrac{N^2 d^2}{M}\big)$ given SRAM size $M$ (typical $d\ll\sqrt{M}$), and the paper proves a lower bound showing optimality up to constants over SRAM regimes. (Thm. 2 & Prop. 3; proof sketch **pp. 23–24**.)
- **Measured IO vs time**: GPT-2-medium config—HBM traffic falls **~9×** (40.3 GB → 4.4 GB), attention runtime **~5.7×** (41.7 ms → 7.3 ms). (**Fig. 2 left, p. 6**.)
- **End-to-end training speed**:
  - **BERT-large (L=512)**: **15%** faster than MLPerf 1.1 record. (**Table 1, p. 7**.)
  - **GPT-2** small/medium: up to **3.5× / 3.0×** faster than HuggingFace; up to **1.7×** vs Megatron-LM. (**Table 2, p. 8**.)
  - **LRA**: vanilla Transformer + FlashAttention runs **2.4×** faster than standard attention; block-sparse FA **2.8×**. (**Table 3, p. 8**.)

- **Quality from longer context**:
  - GPT-2 with FA at **4k** context is still **1.3×** faster than Megatron at **1k** and gets **0.7** better ppl. (**Table 4, p. 8**.)
  - Long documents: micro-F1 improves with sequence length; e.g., ECtHR **72.2 → 80.7** at 8k. (**Table 5, p. 9**.)
  - First >chance Transformer on **Path-X** (61.4%) and (with block-sparse FA) **Path-256** (63.1%). (**Table 6, p. 9**.)

- **Scaling characteristics**: Up to **3×** faster than PyTorch attention for $N\le 2\text{k}$; memory grows **linearly** with $N$ and remains **~20×** lower than exact baselines; supports sequences up to **64k**. (**Fig. 3, p. 9–10**.)

### Strengths

- **Targets the real bottleneck** (HBM IO) with a principled, provably efficient algorithm, not a model change.
- **Exact attention** (no quality trade-off), yet enables **longer contexts** that _do_ improve quality.
- **Solid theory + systems**: IO bounds, correctness, and thorough engineering (fused kernel, careful backward).
- **Useful primitive**: cleanly extends to **block-sparse**; results are strong and practical across GPUs (see App. E speedups).

### Improvements (what could be better)

- **Portability/ergonomics**: Requires custom CUDA per variant; the authors themselves call for a Halide-style compiler path (Sec. 5).
- **Head-dim sensitivity**: Speedups shrink as (d) grows (SRAM pressure; **Fig. 6, p. 29**).
- **Crossover clarity**: For very long contexts some approximate methods can win on raw runtime; clearer guidance on regimes (beyond Fig. 3’s crossover at ~512–1024) would help.
- **Sparse patterns**: Block-sparse results use fixed patterns; exploration of learned/structured sparsity vs accuracy would strengthen the case.

### What I Learned / Liked

- The **online softmax trick** with running $(m,\ell)$ lets you tile exact softmax safely—simple and powerful.
- **RNG-state replay** to avoid storing dropout masks at (O(N^2)) is a neat backward detail.
- The paper reframes “efficient Transformers”: it's not (only) FLOPs—it’s **bytes moved**.

### Summary Brief

FlashAttention reorganizes attention to **stream blocks through SRAM**, computing softmax **incrementally** and **recomputing** during backward so the $N\times N$ matrix never touches HBM. This slashes memory traffic (and thereby wall-clock time) while keeping attention **exact**. Theory shows fewer HBM accesses than standard attention and optimality across SRAM sizes; practice shows **15%–3×** end-to-end speedups, **linear memory** in $N$, and new long-context capabilities (Path-X/256). A block-sparse variant cuts IO further, scaling to **64k** tokens. The main gap is tooling—getting IO-aware kernels without hand-written CUDA.
