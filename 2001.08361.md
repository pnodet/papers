# Scaling Laws for Neural Language Models

[arxiv.org/2001.08361](https://arxiv.org/abs/2001.08361)

### Problem / State of the Art

The paper asks a blunt question: **How does language-model performance scale** with (1) parameters, (2) data, and (3) training compute? Prior work showed big Transformers work well, but gave little guidance for _how much bigger_ and _what to scale first_. This work maps the terrain with **empirical power laws** across many orders of magnitude, aiming to turn “bigger is better” into quantitative recipes.
_Visuals:_ Figure 1 (p.3) shows three near-linear log-log plots: loss vs compute, dataset size, and parameters, each with straight power-law trends when the other two aren’t bottlenecks. The figure caption crystallizes the “scale all three in tandem” message.

### Goal

Derive **simple, predictive scaling laws** for cross-entropy loss $L$ as functions of model size $N$, dataset size $D$, and _optimally allocated_ training compute $C_{\min}$; then use them to **optimize budget allocation** (how big the model should be, how long to train, and how much data to use) for best loss at fixed compute (Sec. 1.2, Eqs. 1.1–1.3; Table 4–6, p.20).

### Challenges

- **Overfitting vs. capacity:** Holding (D) fixed while growing (N) (or vice versa) yields diminishing returns / overfitting; need a joint law for (L(N,D)). (Sec. 4; Fig. 9, p.11.)
- **Batch-size/time trade-offs:** Training runs used non-optimal batch sizes; comparing models fairly requires normalizing to the **critical batch size** and deriving a universal “minimal steps” notion $S_{\min}$. (Sec. 5.1; Fig. 10, p.12.)
- **Theory gap & small-data regime:** Results are empirical; theory is tentative, and fits degrade for tiny datasets. (App. C, p.22.)

### Key Mechanism (What they actually did)

1. **Run lots of LMs** (Transformers 10³–10⁹ non-embedding params) on WebText2 variants, scanning (N), (D), steps, and shapes. (Sec. 2–3.)
2. **Fit power laws** for isolated limits:
   - $L(N) = (N_c/N)^{\alpha_N}$ (Eq. 1.1)
   - $L(D) = (D_c/D)^{\alpha_D}$ (Eq. 1.2)
   - $L(C_{\min}) = (C_c^{\min}/C_{\min})^{\alpha_C^{\min}}$ (Eq. 1.3)

3. **Unify** (N) and (D) with a joint law capturing overfitting:
   [
   $L(N,D)=\big[(N_c/N)^{\alpha_N/\alpha_D} + (D_c/D)\big]^{\alpha_D}$
   ]
   (Eq. 1.5; Fig. 4-left, p.5.)
4. **Normalize training dynamics** using the **critical batch size** $B_{\text{crit}}(L)\propto L^{1/\alpha_B}$ to define the minimal steps $S_{\min}$ and fit **learning curves**:
   [
   $L(N,S_{\min})=(N_c/N)^{\alpha_N}+(S_c/S_{\min})^{\alpha_S}$
   ]
   (Eqs. 1.4 & 1.6; Fig. 4-right, p.5; Fig. 10, p.12.)
5. **Optimize compute allocation** by minimizing $L(N,S_{\min})$ at fixed $C_{\min}=6N, B_{\text{crit}}, S_{\min}$, yielding scaling of the _optimal_ $(N, B, S, D)$ with budget. (Sec. 6; Fig. 14, p.16.)

### Key Results (numbers you can use)

- **Exponents:**
  $(\alpha_N \approx 0.076,\ \alpha_D \approx 0.095,\ \alpha_C^{\min}\approx 0.050,\ \alpha_S\approx 0.76,\ \alpha_B\approx 0.21)$. (Eqs. 1.1–1.4; Table 5, p.20.)
- **Overfitting law & data-with-size rule:** From Eq. 1.5, to keep penalties small, scale data **sublinearly** with model size: $D \propto N^{\alpha_N/\alpha_D}\approx N^{0.74}$. (Sec. 1.2; Fig. 9-right, p.11.)
- **Critical batch size:** $B_{\text{crit}}(L)\approx B_! L^{1/\alpha_B}$, with $B_!\approx 2\times 10^8$ tokens. (Eq. 1.4; Fig. 10, p.12.)
- **Compute-efficient frontier:** At fixed budget, the optimal choices scale as
  - $N\propto C_{\min}^{0.73}$,
  - $B\propto C_{\min}^{0.24}$,
  - $S\propto C_{\min}^{0.03}$ (i.e., **steps barely grow**),
  - and $L(C_{\min})\propto C_{\min}^{-0.050}$. (Eqs. 1.7–1.8; Fig. 14, p.16; Fig. 13, p.15.)

- **Sample efficiency:** Bigger models reach the same loss **with fewer steps and fewer tokens** (Fig. 2, p.4; Fig. 19, p.24).
- **Architecture shape matters weakly:** Depth/width/head count barely affect loss at fixed non-embedding (N). (Fig. 5–6, p.8.)
- **Generalization offsets:** Loss on other corpora tracks in-distribution loss with a near-constant offset. (Fig. 8, p.10.)
- **“Train big, stop early”:** Compute-efficient runs stop ~10% above convergence (derived in App. B). (p.21.)

### Strengths

- **Clarity & reach:** Straight-line log-log trends over **6–8 orders of magnitude**; easy-to-apply formulas with reported constants. (Fig. 1, p.3; Table 5–6, p.20.)
- **Actionable knobs:** Concrete rules for **how to spend GPU budget**: prioritize **parameters**, bump **batch size**, barely extend **steps**. (Fig. 3–4, p.4–5; Fig. 14, p.16.)
- **Unified view of overfitting:** The (L(N,D)) law cleanly predicts when scale hits data limits and how to remedy it. (Sec. 4; Fig. 9, p.11.)
- **Robust to shape/hparams:** Weak sensitivity to architecture shape and LR schedule simplifies planning. (Fig. 5–6, p.8; Fig. 22–23, p.26.)

### Improvements / Caveats

- **Single domain/tokenizer:** Laws are tied to WebText2 and a specific BPE; constants (N_c,D_c) are dataset/tokenization-dependent. (Sec. 1.2 notes; p.4–5.)
- **Non-optimal training in the raw data:** Many runs used fixed batch sizes; the compute law needs adjustment to $C_{\min}$ (the paper does this ex-post). (Sec. 3.3 & 6; Fig. 13, p.15.)
- **Small-data regime:** Fits degrade for very tiny (D) (epochs ~40 steps); regularization/augmentation not deeply explored. (Sec. 4.2; App. C, p.22–23.)
- **Theory is nascent:** Authors emphasize the lack of a solid theory for the exponents and why they’re so stable. (App. C, p.22.)

### What I Learned / Liked

- The **compute-efficient scaling** is counter-intuitive and practical: spend budget mostly on **bigger models**, increase **batch**, and **don’t chase full convergence**—you gain speed _and_ sample efficiency. (Fig. 3–4, p.4–5; Fig. 14, p.16.)
- The **overfitting universality** $D\sim N^{0.74}$ is a tidy rule of thumb for planning future runs and dataset growth. (Sec. 1.2; Fig. 9, p.11.)
- Normalizing training with $B_{\text{crit}}$ and $S_{\min}$ gives a clean, almost **model-size-independent** learning curve law. (Sec. 5; Fig. 10, p.12; Fig. 11, p.14.)

### Summary Brief

This paper shows that **LM loss follows smooth power laws** in parameters $N$, data $D$, and compute $C_{\min}$. It introduces a **joint law** $L(N,D)$ that explains overfitting and a **training-time law** $L(N,S_{\min})$ grounded in the **critical batch size**, then uses them to derive **compute-optimal recipes**: scale **model size fast** ($N\propto C_{\min}^{0.73}$), scale **batch moderately**, and **steps hardly at all**, stopping **well before convergence**. The result is a practical blueprint for planning large-scale LM training with predictable returns—and a pointer to where today’s scaling will eventually hit data-limited walls. (See Fig. 1–4, 10, 13–14; Tables 4–6, p.20.)
