# Variational Continual Learning

[arxiv.org/1710.10628](https://arxiv.org/abs/1710.10628)

### Problem / State of the Art

Deep nets trained sequentially tend to catastrophically forget earlier tasks. Prior fixes like EWC, SI, and Laplace-style penalties regularize parameter drift but are heuristic and require tuned hyperparameters. The paper argues that **approximate Bayesian inference**—carrying a posterior over weights and updating it online—already gives a general, principled recipe for continual learning across both discriminative and generative models. (See the intro and Sec. 2; multi-head architectures are sketched in _Fig. 1_ on p.4.)

### Goal

Introduce **Variational Continual Learning (VCL)**: fuse online variational inference with Monte-Carlo VI for neural networks, so each step does a Bayes-style update (previous posterior × likelihood → new posterior). Extend it with a tiny **episodic memory (coreset)** to further curb forgetting. (Sec. 2–3; Algorithm 1, p.3.)

### Challenges

- Balancing plasticity vs. stability without manual λ’s.
- Intractable exact posteriors for neural nets; repeated approximations can accumulate error.
- Handling **task emergence** (new heads) and **task evolution** under one training scheme.
- Bringing the approach to **generative models** (VAEs) where naive fine-tuning forgets badly. (Sec. 2–4; _Fig. 6–7_ illustrate generative forgetting/metrics, pp.10–11.)

### Key Mechanism

- **Projection view of online Bayes:** at step $t$, solve $q_t(\theta) = \arg\min_{q\in Q} \mathrm{KL}\big[q(\theta)\,\|\,q_{t-1}(\theta)\,p(D_t\mid\theta)/Z_t\big]$. With mean-field Gaussian $q$, this becomes maximizing
  $\sum_{n=1}^{N_t}\mathbb{E}_{q_t}\big[\log p(y_t^{(n)}\mid x_t^{(n)},\theta)\big] - \mathrm{KL}\big(q_t\,\|\,q_{t-1}\big)$
  —a single objective that both fits the new data and penalizes movement from the previous posterior. (Eq. 1 & 4.)
- **Coreset VCL:** keep a tiny set of representative points $C_t$. Propagate with non-coreset data to $\tilde q_t$, then **fold in the coreset only for prediction**: $q_t = \mathrm{proj}(\tilde q_t , p(C_t\mid\theta))$. See **Algorithm 1** (p.3).
- **Architectures:** Multi-head discriminative nets and a multi-head VAE where heads generate intermediate features from _z_ while the decoder to _x_ is shared (Fig. 1b, p.4).
- **VAE version:** add a **$\mathrm{KL}(q_t(\theta)\,\|\,q_{t-1}(\theta))$** term to the standard ELBO to update a posterior over generator parameters; encoders can be task-specific. (Sec. 4.)

### Key Results

**Discriminative**

- **Permuted MNIST (single-head):** after 10 tasks, VCL ≈ **90%** avg acc; EWC ≈ **84%**, SI ≈ **86%**, LP ≈ **82%**. Adding a 200-point coreset per task lifts VCL to ≈ **93%**; larger coresets (5k) reach **95.5%** (Figs. 2–3, p.7).
- **Split MNIST (multi-head):** VCL ≈ **97.0%** avg; SI ≈ **98.9%**; EWC **63.1%**; LP **61.2%**. Coreset lifts VCL to ≈ **98.4%** (Fig. 4, p.8).
- **Split notMNIST (deeper nets):** VCL ≈ **92%**, SI ≈ **94%**, EWC **71%**, LP **63%**; VCL + random coreset ≈ **96%** (Fig. 5, p.9).

**Generative (VAE)**

- Naive online fine-tuning **forgets** (cannot generate earlier classes). VCL and SI **retain** earlier digits/letters with strong sample quality (Fig. 6, p.10).
- Quantitatively, across **test log-likelihood** and **classifier uncertainty**, **VCL is on par with or slightly better than SI**, and better than EWC/LP overall (Fig. 7, p.11).

### Strengths

- Clean **Bayesian principle**: the KL to $q_{t-1}$ emerges from VI—no ad-hoc penalties.
- **Hyperparameter-free objective** (no λ to sweep), yet competitive or better than tuned baselines.
- Works for both **discriminative and generative** settings; integrates a tiny **episodic memory** (coreset) elegantly.
- **Multi-head flexibility** and incremental head growth (new tasks start from the prior). (Sec. 3–4; Figs. 1–3.)

### Improvements

- **Mean-field Gaussian** posterior can under-estimate uncertainty; richer posteriors (full-covariance, flows) might help.
- **Coreset selection** is heuristic (random or K-center); learning-based or uncertainty-aware selection could be stronger.
- **Benchmarks** are small (MNIST/notMNIST); scaling to ImageNet-level, transformers, or RL would test practicality.
- **Task-labels & multi-head** assumption simplify routing; single-head class-incremental with unknown task IDs is harder.
- Generative setup uses **task-specific encoders**; sharing/adapter strategies weren’t explored. (Sec. 4, 6; Figs. 4–7.)

### What I Learned / Liked

- The **projection** view $q_t \approx \mathrm{proj}(q_{t-1}\cdot \text{likelihood})$ makes continual learning feel like standard VI with a moving prior.
- The **two-stage coreset scheduling**—propagate without, then inject coreset only for prediction—is a neat, minimalist way to refresh memory.
- Seeing the **generative** case treated symmetrically (ELBO + KL to $q_{t-1}$) is conceptually satisfying. (Algorithm 1; Fig. 6–7.)

### Summary Brief of the Paper

**Variational Continual Learning (VCL)** casts continual learning as online variational Bayes: at each step, learn a new posterior over network weights by maximizing data fit while staying close to the previous posterior. A small **coreset** further reduces forgetting. On Permuted/Split MNIST and notMNIST, VCL matches or beats EWC/LP and is competitive with SI, **without hyperparameter tuning**; for VAEs, it avoids catastrophic forgetting and yields strong likelihood/uncertainty metrics. The method is simple, extensible, and unifies discriminative and generative continual learning under one Bayesian framework. (See Sec. 2–6; Figs. 1–7.)
