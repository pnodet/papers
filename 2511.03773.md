# Scaling Agent Learning via Experience Synthesis

[arxiv.org/2511.03773](https://arxiv.org/abs/2511.03773)

### Problem / State of the Art

Training LLM-based agents with RL is hampered by four gnarly constraints: (1) rollouts are expensive and slow, (2) task suites are shallow and static, (3) reward signals are sparse/noisy/unstable in dynamic settings like the web, and (4) RL infrastructure (VMs, Dockerized envs) is heavy and brittle—making scalable, diverse, RL-ready experience collection the bottleneck. The paper proposes **DreamGym**, a unified framework that **synthesizes** experience—rather than harvesting it from heterogenous real environments—to make agent RL practical and scalable. (Overview and motivation on pp.1–2; Fig. 1 contrasts paradigms.)

### Goal

Provide a _single_ scalable pipeline that:

- distills environment dynamics into an **LLM “experience model”** operating in an abstract textual state space;
- supplies **consistent transitions + reward signals** via reasoning;
- **curriculum-generates** new, challenging tasks;
- and supports **purely synthetic RL** as well as **sim-to-real warm starts**. (Fig. 2 on p.4; §4.)

### Why This Is Hard (Challenges)

- Real envs are costly per step, long-horizon, and reward-sparse/unstable; many aren’t RL-ready (no resets, safety risks).
- Fixed task sets don’t scale exploration; generating/validating new tasks is expensive.
- Infrastructure heterogeneity throttles throughput. (§1–2.)

### Key Mechanism (How it works)

- **Reasoning-based Experience Model (Mexp):** an LLM predicts next abstract state + reward using CoT, conditioned on current state/action, task, history, and **top-k retrieved similar experiences** from a replay buffer. Trained by SFT to generate reasoning traces _and_ next-state predictions (Eq. 5). (§4.1; Fig. 2.)
- **Experience Replay Buffer:** seeded with offline real trajectories; continuously updated with synthetic rollouts to stabilize and diversify transitions aligned with the evolving policy. (§4.1.)
- **Curriculum Task Generator (Mtask):** reuses the same model to spawn **task variations** prioritized by **group reward entropy**—i.e., tasks with mixed success/failure are selected as maximally informative. (§4.2; Eq. 7.)
- **Training Loops:** plug-and-play with PPO/GRPO entirely in synthetic space; optional **DreamGym-S2R** transfers to real envs after synthetic pretraining. (§4.3.)
- **Theory:** a trust-region bound shows real-env improvement if synthetic surrogate gain beats (i) KL penalty and (ii) experience-model error terms—**reward accuracy** (ε_R) and **transition consistency** (ε_P). (Appendix B, Theorem 1.)

### Key Results (Evidence)

- **Non-RL-ready WebArena:** Purely synthetic DreamGym **beats all baselines by >30%** and makes RL training feasible where real-env RL struggles. (p.2 summary; Table 1 p.7 shows, e.g., DreamGym GRPO 13.3% vs traditional 7.3% on L3.2-3B.)
- **RL-ready WebShop / ALFWorld:** Pure synthetic training **matches** PPO/GRPO trained on **80k real transitions** (Table 1).
- **Sim-to-Real (S2R):** Pretrain in DreamGym, then do **small RL phase (5k real transitions)** → **>40% performance gains** vs training from scratch, while using **<10%** of external data. (p.2 summary; Table 1 “DreamGym-S2R”.)
- **Efficiency:** On WebArena, DreamGym attains strong success rates at roughly **⅓–⅕** of training effort (rollout + GPU hours) compared to real-env RL (Fig. 3, left, p.8).
- **Ablations:** Remove task generation / replay / reasoning → notable drops (Table 2 p.9). History + reasoning improve **consistency, informativeness, and reduce hallucination** (Fig. 4 p.9; judged by GPT-4o). Data efficiency shown in Fig. 5 (p.10). A qualitative trajectory appears in Fig. 6 (p.10).

### Strengths

- **Pragmatic reframing:** Treat environments as **generators of structured, reasoning-rich experiences** instead of exact simulators—reducing cost while preserving learning signals. (§4, Conclusion.)
- **Unified toolchain:** Interaction + replay + curriculum + RL in one loop; portable across PPO/GRPO. (Fig. 2.)
- **Compelling S2R story:** Real gains with tiny real-data budgets—high leverage for expensive domains. (Table 1.)
- **Theory aligns with design:** Bound emphasizes **reward correctness** and **domain-consistent transitions**, not pixel-level fidelity—matching the abstract-state approach. (Appendix B.)
- **Compute & data efficiency:** Experience model trains with modest offline sets (e.g., WebArena uses ~4.8k trajectories from leaderboards + random/oracle; Appendix A.3), yet yields stable policy learning.

### Improvements (What could be stronger)

1. **Reward calibration & auditability.** Rewards are outcome-based (final-step r = 1/0). Intermediate shaping and **systematic auditing of ε_R** (mis-scored successes/failures) would help, ideally with human spot-checks or secondary automated verifiers. (§4.1.1 adopts outcome-based rewards.)
2. **Domain-gap measurement.** Provide quantitative probes of **ε_P** (transition consistency) beyond end metrics—e.g., state-distribution divergence vs real env, failure typologies, and reset-edge cases. (Theory defines ε_P, but empirical proxies are not deeply reported.)
3. **Generalization breadth.** Cross-domain transfer weakens on big modality gaps (web → ALFWorld). A multi-env “unified world model” (noted in Limitations) would test scalability of the abstract state space. (Limitations p.11; Fig. 3 middle shows transfer patterns.)
4. **Evaluator dependence.** Part of the experience-quality assessment uses **GPT-4o** as judge (Fig. 4). Adding human eval and open baselines (e.g., rule-based checkers) would reduce evaluator bias.
5. **Reproducibility knobs.** More sensitivity studies on: replay retrieval (encoder choice, k), curriculum **λ** (synthetic-task mix), and history window length. (§4.2 introduces λ but large sweeps aren’t shown.)
6. **Compute footprint transparency.** While cheaper than real-env RL, the appendix lists sizable clusters (A100/H100 nodes). A cost-per-point or kWh analysis would ground the scalability claim. (Appendix A.)
7. **Safety/irreversibility simulation.** Since a motivation is avoiding unsafe real actions, include stress tests simulating irreversible operations and measuring conservative behavior learned in DreamGym vs real env. (Motivation §1.)

### What I Learned / Liked

- The **abstract textual state space** is a sweet spot: it preserves causality and action semantics while stripping HTML/GUI noise—yielding **token-efficient** learning signals.
- The **reward-entropy curriculum** is a neat, low-overhead selector that pushes the agent at the “right” difficulty.
- The **theory** cleanly explains why realism isn’t necessary; **correct signals + consistent transitions** suffice for policy improvement.

### Summary Brief

**DreamGym** replaces expensive, brittle real-env rollouts with a **reasoning-driven experience model** that simulates **abstract states** and **rewards**, augments learning with **replay retrieval**, and **auto-generates** challenging tasks via **reward-entropy**. In WebArena (non-RL-ready), it **surpasses baselines by >30%** with **purely synthetic** training; in WebShop/ALFWorld, it **matches** PPO/GRPO trained on **80k** real steps; and with **DreamGym-S2R**, a **small real-RL phase** after synthetic pretraining yields **>40%** gains with **<10%** of external data. A trust-region bound justifies why optimizing for **reward correctness** and **transition consistency** in the synthetic MDP yields real-world policy improvement. The approach is a strong step toward scalable, general-purpose RL for agents—though future work should deepen reward/transition audits, broaden multi-env generalization, and further de-bias evaluation.
