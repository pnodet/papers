# Reflexion: Language Agents with Verbal Reinforcement Learning

[arxiv.org/2303.11366](https://arxiv.org/abs/2303.11366)

### Problem / State of the Art

LLM-based agents can already act in external environments (games, APIs, web) but don’t learn efficiently from trial-and-error: classic RL needs many samples and finetuning, while in-context prompting alone doesn’t adapt over episodes. The paper targets this gap: helping language agents _learn from mistakes_ without weight updates.

### Goal

Introduce **Reflexion**—a lightweight “verbal reinforcement” framework where an agent converts feedback into _textual self-reflections_ stored in episodic memory, then conditions future trajectories on that memory to improve decision-making, reasoning, and coding performance.

### Challenges

- **Credit assignment in language space:** turning sparse/binary rewards into actionable guidance.
- **Generating useful, concise reflections:** the self-reflection must pinpoint the failure and suggest fixes.
- **Memory limits / context budgets:** only a few reflections (typically 1–3) fit reliably.
- **Evaluation noise for coding:** flaky or incomplete self-generated tests can mislabel solutions. (Table 2 on p. 8 analyzes FP/FN patterns.)
- **Local minima / exploration:** on WebShop, agents didn’t improve over trials, suggesting Reflexion struggles when success needs highly diverse exploration strategies (p. 14, Fig. 6).

### Key Mechanism

- **Three models** (p. 4, Fig. 2): **Actor** (generates actions/thoughts), **Evaluator** (scores trajectory—via heuristics, EM grading, or an LLM), **Self-Reflection** (LLM that turns feedback + trajectory into a succinct “what to do next time” note). These notes go to **long-term memory**; the current trajectory serves as **short-term memory**.
- **Loop (Algorithm 1, p. 4):** generate trajectory → evaluate → produce reflection → append to memory → retry until success or max trials. Memory is capped (usually 1–3 reflections).
- **Task-specific evaluators:**
  - _Decision-making (ALFWorld):_ heuristic or LLM binary signals (e.g., detect cycles/inefficient planning).
  - _Reasoning (HotPotQA):_ exact-match grading between trials; ablate episodic memory vs self-reflection. (Fig. 4, p. 7.)
  - _Programming:_ generate _self-written unit tests_ (up to 6), AST-filter them, run, then reflect to edit code; pass@1 remains valid. (Sec. 4.3, Table 1 p. 7.)

### Key Results

- **ALFWorld (sequential decision-making):** ReAct+Reflexion solves **130/134** tasks, a **~22% absolute** improvement over baseline across 12 trials; halts for ReAct-only around trial 6–7. (Fig. 3, p. 6; example trajectory on p. 13.)
- **HotPotQA (reasoning):** Reflexion boosts both CoT and ReAct settings; CoT with ground-truth context still gains **+14%** via self-reflection; episodic memory alone underperforms self-reflection (Fig. 4a–c, p. 7).
- **Programming:**
  - **HumanEval (Python):** **91% pass@1**, surpassing GPT-4’s reported 80%.
  - **HumanEval (Rust):** **68%** vs GPT-4 60%.
  - **MBPP (Rust):** **75.4%** vs 70.9% GPT-4; **MBPP (Python)** slightly _below_ GPT-4 (77.1% vs 80.1%).
  - **LeetcodeHardGym (Python):** **15%** vs GPT-4 7.5%. (Table 1, p. 7.)

- **Why MBPP(PY) lags:** higher **false-positive** rate from internal tests (16.3% vs 1.4% on HumanEval), causing premature “success.” (Table 2, p. 8; analysis p. 8.)
- **Ablations:** Removing self-reflection or tests hurts coding (Table 3, p. 8). Reflexion also shows consistent gains across weaker/stronger base LLMs (Appendix A, p. 12).

### Strengths

- **Lightweight learning** without finetuning; simple to bolt onto existing agents.
- **Interpretable memory:** reflections are readable “lessons learned.” (Examples on pp. 13, 17–19.)
- **General across tasks** (acting, reasoning, coding) and **LLMs** (GPT-3.5/4, text-davinci-003).
- **State of the art** on several code benchmarks with **pass@1** eligibility preserved.

### Improvements

- **Test reliability:** incorporate mutation testing, differential testing, or fuzzing to curb false positives; maintain a _validated_ seed test pool per task. (Motivated by Table 2’s FP analysis.)
- **Richer memory:** beyond a sliding window—learned retrieval, vector DBs, or structured schemas to avoid forgetting and reduce prompt bloat. (Limitation noted in Sec. 5.)
- **Exploration strategies:** add stochastic/planned exploration or planner–critic loops to escape local minima (WebShop failure case, p. 14).
- **Evaluator robustness:** combine heuristic + LLM graders; calibrate with confidence; consider ensemble or cross-check evaluators.
- **Report costs:** latency and token overhead of multi-trial loops and memory prompts would help practitioners assess trade-offs.

### What I Learned / Liked

The neat idea is to treat _language_ as the update rule: turn “fail/succeed” into a compact lesson the model reuses. The paper’s figures make that tangible—e.g., Fig. 2’s loop (p. 4) and Fig. 3–4’s learning curves showing improvement _across trials_ rather than per-sample.

### Summary Brief

**Reflexion** augments LLM agents with a verbal self-improvement loop: after each attempt, an evaluator yields a simple score or signal; a separate LLM distills that into a short reflection stored in memory; the actor conditions on those reflections next time. This boosts performance across decision-making (ALFWorld, large gains over ReAct), multi-hop QA (HotPotQA, +14% even with GT context), and coding (new SOTA pass@1 on HumanEval and others), while exposing limits around flaky tests, memory size, and exploration-heavy environments. The core contribution is pragmatic and effective: _learning by talking to yourself—and remembering it._
