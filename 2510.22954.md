# Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)

[arxiv.org/2510.22954](https://arxiv.org/abs/2510.22954)

## Problem / State of the Art

LLMs are widely used for **open-ended tasks** (creative writing, brainstorming, advice), but there’s growing concern that they:

- Produce **homogeneous, stereotyped outputs** instead of rich human-like diversity.
- Might cause **“homogenization of human thought”** as billions of users see similar completions over and over.

Existing work on diversity and creativity:

- Mostly uses **narrow or synthetic tasks** – random numbers/names, short poetic prompts, or controlled creativity benchmarks (e.g., TTCT-style tasks).
- Often evaluates **one model at a time**, with limited prompts, and without grounding in **real user queries**.
- Alignment datasets emphasize **correctness and safety**, typically with **one “best” response**, not multiple equally valid ones.

So we lack:

- A **large-scale, real-world benchmark** of open-ended prompts.
- Systematic evaluation of **intra-model** (same model, multiple samples) and **inter-model** (different models) diversity.
- Grounded data on **human preference distributions** over multiple plausible answers.

---

## Goal

The paper aims to:

1. **Create a benchmark** (INFINITY-CHAT) for **real-world open-ended prompts** with a taxonomy of query types.
2. Empirically characterize the **“Artificial Hivemind”** effect:
   - **Intra-model repetition**: same model, many samples → similar responses.
   - **Inter-model homogeneity**: different models → similar responses.

3. Study how **LMs, reward models, and LM judges** evaluate **alternative responses** that humans see as similarly good, or where humans **disagree**.
4. Provide tools and evidence to guide **pluralistic alignment**—models that support diverse human values and creativity.

---

## Challenges

- **Defining open-endedness**: deciding which queries _truly_ admit multiple reasonable answers vs. having a single correct one.
- **Ecological validity**: synthetic prompts don’t reflect how people actually use chatbots; the authors want **in-the-wild** data.
- **Taxonomizing queries**: open-ended prompts cover a huge variety of intentions (stories, planning, philosophy, etc.), which must be systematically categorized.
- **Measuring diversity**:
  - Need metrics that work across **thousands of prompts and dozens of models**.
  - Must go beyond exact string overlap to **semantic similarity**.

- **Capturing human disagreement**:
  - Open-ended responses often have **no consensus “best” answer**.
  - Need many labels per example to see **distributions**, not just majority votes.

- **Evaluating evaluators**:
  - Reward models and LM judges are usually trained assuming **one correct preference**; checking their behavior under **pluralism** is non-trivial.

---

## Key Mechanism

### 1. INFINITY-CHAT Dataset

**Source & filtering**

- Start from **WildChat**: 1M+ real ChatGPT logs.
- Filter to 37k+ GPT-4 queries that are:
  - English, non-toxic, and single-turn.
  - Moderate length (15–200 characters).

- Use GPT-4o to classify each query along:
  - Is it **meaningful**?
  - Is it **greeting/model inquiry**?
  - Does it expect a **single** vs **multiple** valid responses?

- After filtering & light rewriting for clarity:
  - **26,070 open-ended** queries.
  - **8,817 closed-ended** queries.

**Taxonomy construction**

- Start with ~100 sample queries and **manually** label / group them into a hierarchy.
- Scale to full dataset with GPT-4o applying and extending categories.
- Final taxonomy: **6 top-level** and **17 sub-categories**, including:
  - **Creative Content Generation** (58.0%)
  - **Brainstorm & Ideation** (15.2%)
  - **Speculative & Hypothetical Scenarios** (22.2%)
  - **Analytical & Interpretive Questions** (22.6%)
  - **Skill Development** (23.5%)
  - **Concept Explanations**, **Personal Advice**, etc.

A human study confirms that the sampled subset **really is open-ended**: 89% judged open-ended by majority vote; 100% had at least one annotator saying so.

---

### 2. Measuring the “Artificial Hivemind”

**Setup**

- Pick **INFINITY-CHAT100**: 100 representative open-ended prompts, human-verified.
- For each of **70+ LMs** (25 highlighted):
  - Sample **50 responses per query** with:
    - Top-p = 0.9, temperature = 1.0 (and separately a min-p = 0.1, T = 2.0 setup).

  - Compute **sentence embeddings** with text-embedding-3-small.
  - Use **cosine similarity** to analyze:
    - **Intra-model**: pairwise sim within same model’s 50 responses per prompt.
    - **Inter-model**: pairwise sim across different models’ responses.

Visually (e.g., **Figure 1**), they show clustering: for “Write a metaphor about time,” responses from **25 models × 50 samples** cluster into mainly **two concepts**:

- “Time is a river…”
- “Time is a weaver/tapestry…”

Despite huge freedom, models converge on a few clichés.

---

### 3. Dense Human Annotations

They collect **two complementary annotation types** on model responses for 50 prompts:

1. **Absolute ratings**
   - 15 responses per prompt.
   - 25 annotators per (Query, Response).
   - 1–5 quality scale → **18,750 ratings**.

2. **Pairwise preferences**
   - 10 response pairs per prompt.
   - 25 annotators per (Query, R1, R2).
   - Labels: strong/weak preference, tie → **12,500 labels**.

They compute **Shannon entropy** over label distributions to quantify **disagreement**: many prompts/responses show **high entropy**, meaning annotators don’t agree which answer is better.

---

### 4. Evaluating LMs, Reward Models, and LM Judges

For each annotated item, they compute model scores:

- **LM scores**: perplexity-based scores of the response given the query.
- **Reward models**: scalar outputs from 6 strong RM models (e.g., from RewardBench).
- **LM judges**: scalar ratings from 4 judge LMs (e.g., GPT-4o, Prometheus) using:
  - Overall quality rubric.
  - HHH (Helpful, Harmless, Honest) rubric.

Then they compute **correlations** (Pearson/Spearman) between **average human scores** and **model scores**, focusing on:

- **Similar-quality subsets** (where humans rate responses similarly).
- **High-disagreement subsets** (high entropy or high preference disagreement).

---

## Key Results

### 1. Taxonomy & Real-World Usage

- **Creative Content Generation** dominates (≈58% of open-ended queries), but many other types are common:
  - **Alternative writing genres** (38.5%),
  - **Concept explanations** (23.6%),
  - **Skill development** (23.5%),
  - **Analytical & interpretive questions** (22.6%),
  - **Hypotheticals** (22.2%).

- They identify **314 novel categories** beyond the seed taxonomy (e.g., ethical, cultural, media-related queries).

### 2. Intra-Model Repetition

- With standard top-p decoding (0.9, T=1.0):
  - For many strong LMs, **average pairwise similarity among responses to the same prompt often > 0.8**.
  - On average, **79% of prompts** have intra-model similarity ≥ 0.8 (exact percentages vary per model; see heatmap on page 4).

- As a baseline, randomly paired responses from the **global pool** fall in **0.1–0.2 similarity**, so this is not an artifact of the embedding space.

- With **min-p decoding** (T=2.0, min-p=0.1), repetition lessens but remains strong:
  - **81% of pairs still > 0.7** similarity.

### 3. Inter-Model Homogeneity

- Average similarity **between different models’ responses** for the _same prompt_ is also high, roughly **0.71–0.82**, across many model pairs.
- Some particularly high pairs:
  - DeepSeek-V3 ↔ Qwen-max (≈0.82)
  - DeepSeek-V3 ↔ GPT-4o (≈0.81).

- They show **verbatim or near-verbatim overlaps** across models:
  - Identical motto: “Empower Your Journey: Unlock Success, Build Wealth, Transform Yourself.”
  - Highly similar ad copy for an iPhone case collection (same phrases like “Elevate your iPhone,” “bold, eye-catching designs”).

- When clustering the **top-N most similar responses** per prompt:
  - For N=50, you’d expect them all from one model if intra-model diversity dominated.
  - Instead, the average top-50 cluster contains **≈8 distinct models**, meaning **different models frequently generate near-duplicates**.

### 4. Paraphrased Prompts

- For 30 prompts × 4 paraphrases × 42 models:
  - **Within-prompt similarity** ≈ 0.821.
  - **Across-paraphrase similarity** ≈ 0.781.

- So even paraphrasing the prompt doesn’t break the homogenization much.

### 5. Human Preferences & Model Calibration

- Human annotations show **substantial disagreement**:
  - Many (Query, Response) pairs have **high entropy** over 1–5 ratings.
  - Many (Query, R1, R2) triplets have nearly **uniform preference distributions**—annotators split on which is better.

- When comparing **model scores vs. average human ratings**:
  - **On the full dataset**, correlations are decent (varies by model/judge).
  - **On similar-quality subsets**:
    - Correlations **drop sharply** for LMs, reward models, and judges – they struggle to treat equally good alternatives as equivalent.

  - **On high-disagreement subsets**:
    - Correlations also **decrease significantly**: models don’t reflect the plural human preference distribution and instead pick winners.

Overall, the paper argues that **current evaluation/training pipelines reinforce a single consensus notion of quality**, which under-rewards genuinely diverse alternatives.

---

## Strengths

- **Highly realistic setting**: prompts come from **real users**, not lab-designed tasks.
- **Large scale and rich structure**:
  - 26k open-ended queries, taxonomy of 6×17 categories, plus closed-ended controls.
  - 31,250 human labels with **25 annotators per item**, enabling analysis of **distributions**, not just majority vote.

- **Clear conceptual framing**: the **“Artificial Hivemind”** metaphor captures both:
  - Intra-model mode collapse.
  - Inter-model convergence across different families.

- **Methodologically simple but powerful**:
  - Embedding similarity + clustering is straightforward, yet reveals strong patterns.
  - Entropy-based analysis of disagreement is intuitive and informative.

- **Connects diversity with alignment**:
  - Frames homogenization as a **pluralistic alignment** and **societal** problem, not just a technical curiosity.

- **Actionable future directions**:
  - Suggests using this benchmark for **red-teaming**, **training diversity-aware reward models**, and **decoding strategy evaluation**.

---

## Improvements

Some ways the work could be strengthened or extended:

1. **Causal analysis of the homogenization sources**
   - The paper speculates about shared pretraining data, RLHF pipelines, and synthetic data contamination, but doesn’t pinpoint which contributes how much.
   - Future work with controlled pretraining or access to training data could disentangle these.

2. **Quality–diversity tradeoff characterization**
   - They fix decoding parameters to ensure reasonable quality, but don’t deeply explore **how much quality drops** when you aggressively push for diversity across models / prompts.

3. **Stronger multilingual / cross-cultural coverage**
   - Current dataset is **English-only**, derived mostly from one platform (ChatGPT WildChat logs).
   - A multilingual extension would be important for understanding **cultural homogenization**.

4. **Richer diversity metrics**
   - They primarily use **sentence-level embedding similarity**; other metrics (topic diversity, stylistic variation, discourse structure) could capture more nuanced differences.

5. **Intervention experiments**
   - The paper is mostly diagnostic. Demonstrating **concrete mitigation strategies** (e.g., diversity-aware RL, multi-objective reward models) on their benchmark would make it more prescriptive.

6. **User-level / persona-level differentiation**
   - It would be interesting to measure how models respond to prompts _conditioned on different user personas_ and whether homogenization persists.

---

## What I Learned / Liked

- The **“time is a river” / “time is a weaver”** clustering is a killer visual: an extremely high-dimensional problem (metaphors about time) collapsing to **two conceptual modes** across many models.
- The work makes explicit that **ensemble or swarm approaches don’t automatically guarantee diversity** if all models share similar priors and alignment.
- The dense human-annotation design is neat: by using **25 annotators**, they can clearly see **where humans themselves are pluralistic**, and then show that models (including reward models) often **ignore that pluralism**.
- It crystallizes a central tension: **alignment for safety & helpfulness** versus **alignment for diversity & plurality**—and shows these can come apart in practice.

---

## Summary Brief of the Paper

The paper introduces **INFINITY-CHAT**, a large dataset of **26k real, open-ended user queries** plus a taxonomy of 6×17 query types (creative writing, brainstorming, hypotheticals, analysis, advice, etc.). Using 100 representative prompts and **70+ LMs**, the authors show that both **single models** and **different models across families** tend to produce **highly similar responses**, even under stochastic decoding. They call this cross-model convergence the **“Artificial Hivemind”**.

They further collect **31k+ human ratings** (absolute and pairwise, with 25 annotators each) on alternative responses, and find substantial **human disagreement** about which response is best. Yet **LMs, reward models, and LM judges** are **poorly calibrated** on these contested or similar-quality cases: their scores correlate much less with human averages, suggesting that current training and evaluation pipelines **favor a single narrow notion of quality** and under-reward diverse, equally good answers.

Overall, the paper provides a **real-world benchmark, taxonomy, and diagnostic framework** showing that current LMs, even across families, behave like an **Artificial Hivemind**—posing a long-term risk of **homogenizing human expression** and pointing toward the need for **pluralistic, diversity-aware alignment**.
