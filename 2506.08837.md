# Design Patterns for Securing LLM Agents against Prompt Injections

[arxiv.org/2506.08837](https://arxiv.org/abs/2506.08837)

### Problem / State of the Art

Modern LLM agents read untrusted natural language and can call powerful tools (files, email, web, code, DBs). Prompt injection turns that openness against them, steering actions like data exfiltration or arbitrary code exec. Existing defenses are mostly heuristic (prompting, detection, adversarial training), or generic isolation add-ons that still leave gaps. The authors argue that **general-purpose** agents are unlikely to get strong guarantees with today’s models; instead we should deliberately constrain agents so prompt-injected content can’t trigger consequential actions.

### Goal

Propose a **small set of principled, composable design patterns** that let teams build application-specific agents with _provable_ or at least _arguable_ resistance to prompt injection—balancing utility vs. safety—and show how they apply across ten concrete case studies.

### Challenges

- Untrusted inputs appear everywhere (user prompts, third-party data, emails, files, DBs).
- Tool feedback loops let injected text influence control flow.
- Heuristic filters/detectors raise the bar but don’t give guarantees.
- Human-in-the-loop “confirmations” don’t scale and are error-prone.
- You still need the agent to be useful—over-constrain it and you lose value.

### Key Mechanism (the six design patterns)

The paper’s core is six patterns that enforce isolation between **untrusted data** and **consequential actions**. The figures on pp. 5–9 illustrate each flow clearly.

1. **Action-Selector** – LLM only maps a request to a fixed allow-listed action (or templated command). No feedback from tools back into the LLM; the model never ingests untrusted data. (See Figure 1.)
2. **Plan-then-Execute** – The LLM first commits to a plan (sequence of tool calls) _before_ seeing any untrusted outputs; execution can’t add new actions later. Controls _control-flow_ but not _data-flow_ (parameters can still be poisoned). (Figure 2.)
3. **LLM Map-Reduce** – Dispatch isolated, no-tool sub-agents that each consume a single untrusted document and emit _constrained_ outputs (e.g., boolean, score), then reduce/aggregate these sanitized results. Limits cross-document influence. (Figure 3.)
4. **Dual LLM** – Split into a **privileged** tool-using LLM that never sees untrusted content and a **quarantined** text-only LLM that processes untrusted inputs. Results are passed as **symbolic variables** the privileged LLM manipulates _by reference_; dereferencing happens in a non-LLM orchestrator. (Figure 4.)
5. **Code-then-Execute** – The LLM writes a _program_ (the plan) that calls tools and can spawn unprivileged LLMs for text processing; then the program runs. This generalizes plan-then-execute with a formal artifact. (Figure 5.)
6. **Context-Minimization** – After using a user prompt to pick actions, **drop it from context** so it can’t steer post-processing; similarly strip unnecessary history to prevent prompt-to-LLM leakage. (Figure 6.)

Appendix A compiles best practices that complement these patterns: action sandboxing/least-privilege, strict output formats (e.g., JSON), scoped user permissions, selective user confirmation, and data/action attribution.

### Key Results / Evidence

This is a design-patterns + case-studies paper, not a benchmark paper. Evidence is qualitative: ten end-to-end case studies show how to apply patterns (OS assistant, SQL agent, email/calendar, customer support, booking, product recommender, resume screener, medication leaflet bot, diagnosis intermediary, software-engineering agent). **Table 1 on p. 10** maps each case to the applicable patterns, and each case study spells out a threat model, naïve design, and hardened designs with trade-offs. Quantitative robustness metrics aren’t reported—by design the security argument comes from the _structure_ of the system and its trust boundaries.

### Strengths

- **Clear, actionable abstractions.** The six patterns are simple, composable, and easy to communicate to engineering teams. (The diagrams on pp. 5–9 are especially useful.)
- **Security reasoning through architecture,** not model heuristics—aligns with traditional systems security thinking (CFI, sandboxing, least privilege).
- **Breadth of scenarios.** The ten case studies cover common agent deployments and show realistic threat models and trade-offs. (See Table 1, p. 10.)
- **Pragmatic stance:** acknowledges that fully general agents with strong guarantees are out of reach _now_; focuses on application-specific agents with defined trust boundaries.

### Improvements

- **Empirical validation.** Add measurements: attack success rates vs. patterns, overhead (latency/cost), and residual risk analysis across red-team suites.
- **Compositional guidance.** Provide a decision matrix or threat-to-pattern mapping for faster design choices; include pattern interactions (e.g., Dual LLM + Map-Reduce + Structured Outputs).
- **Developer ergonomics.** Reference concrete libraries/frameworks for constrained decoding, symbolic variable handling, and orchestrator templates; include code snippets for the “symbolic memory” and safe deref.
- **Formal guarantees.** Some patterns hint at control-flow integrity; formalize properties and assumptions (e.g., what the quarantined LLM can/can’t leak) and link to proofs or model checking where feasible.
- **Edge cases.** Discuss covert channels and steganography (e.g., zero-width chars) between quarantined and privileged paths, and how strict formatting / validation blocks them.

### What I Learned / Liked

- Treat **untrusted text like untrusted code**: don’t let it influence control flow or reach tools.
- **Symbolic handles** are a powerful idea—let the planner move variables it can’t read; dereference in trusted code at the last moment.
- **Map-reduce with constrained map outputs** is a clean way to confine document-level injections and still get useful aggregation.

### Summary Brief of the Paper

The paper argues that today’s LLM agents should be **application-specific** and **architected for isolation**, not generalized and then patched with heuristics. It introduces six design patterns—Action-Selector, Plan-then-Execute, LLM Map-Reduce, Dual LLM, Code-then-Execute, and Context-Minimization—that constrain how untrusted data can influence actions. Through ten case studies and clear diagrams, it shows how to apply these patterns to keep utility while sharply reducing prompt-injection risk. The contribution is a practical, security-first blueprint; the next step is empirical and formal validation of these patterns in the wild.
