# Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism

[arxiv.org/1909.08053](https://arxiv.org/abs/1909.08053)

### Problem / State of the Art

Training ever-larger Transformer LMs improves downstream NLP, but single-accelerator memory caps (model weights + optimizer states + activations) become the bottleneck. Existing model-parallel systems like GPipe and Mesh-TensorFlow help, but often require graph/compiler changes or pipeline scheduling with bubbles. The paper tackles how to train multi-billion-parameter Transformers efficiently in “native” PyTorch, without a custom compiler, while preserving strong scaling and accuracy.

### Goal

Build a simple, intra-layer model-parallel recipe for Transformers that (1) fits multi-billion-parameter models in memory, (2) scales well across hundreds of GPUs, and (3) sets new accuracy marks on standard LM and reading-comprehension benchmarks.

### Challenges

- **Memory:** models no longer fit on a single device even with activation checkpointing; optimizer state exacerbates this.
- **Usability:** many prior approaches require rewriting models or relying on specialized compilers/runtimes.
- **Throughput & communication:** minimizing cross-GPU sync inside layers while keeping GPUs compute-bound. (See Figure 4 on _p.5_ showing only four comms per layer.)

### Key Mechanism (How it works)

- **Column/row sharding inside each Transformer block.**
  • **MLP:** first GEMM is **column-parallel** so GeLU is applied independently per shard; second GEMM is **row-parallel**; only one all-reduce in fwd and one in bwd for the block. (The paper even shows a tiny autograd “f” op that just all-reduces in backward.) See Figure 3 on _p.4_.
  • **Self-attention:** shard K/Q/V **by heads** (each head local to one GPU); the output projection is row-parallel. Again, two all-reduces per layer in fwd/bwd total. (Figure 3b on _p.4_.)
- **Vocabulary-wide tricks:** shard the **embedding/logit** matrices over the vocab and **fuse logits with the loss** so you communicate **b×s** scalars instead of **b×s×v** logits—huge comms savings. (Figure 4 on _p.5_ visualizes the per-layer comms.)
- **Duplicate cheap ops:** do dropout/LN/residuals locally on every GPU (no broadcasts), keeping GPUs compute-bound.

### Key Results

- **Scaling/throughput.**
  • Baseline: 1.2B-param model hits **39 TFLOPs** on a single V100 (≈30% of peak).
  • At **512 V100s** (8-way model-parallel × 64-way data-parallel), they sustain **15.1 PFLOPs** with **~76% scaling efficiency** vs the single-GPU baseline. The **Figure 1** plot on _p.2_ summarizes this.
  • Weak-scaling curve (Figure 5 on _p.6_) shows **77%** efficiency for 8-way model-parallel and **74%** for model+data at 512 GPUs.
- **Accuracy / SOTA.**
  • GPT-2-style 8.3B model (trained on 174 GB deduped text) achieves **10.81** ppl on WikiText-103 and **66.5%** on LAMBADA (zero-shot), both SOTA at the time; see **Table 3** on _p.7_.
  • For BERT-style models, they report **monotonic gains up to 3.9B** and SOTA on **RACE test** (**90.9%**; ensemble also reported). Results are summarized in Table 5 (p.8) and text.
- **Engineering setup.** 32 DGX-2H nodes (512 V100 32 GB), NVSwitch (300 GB/s intra-node) + 8× IB per node (100 GB/s).

### Strengths

- **Simplicity with impact:** The scheme uses a few collective ops inserted into PyTorch (no compiler), yet reaches multi-PFLOP training at high efficiency. (Figure 3/4 on _p.4–5_.)
- **Clear comms-minimizing design:** Two all-reduces per layer pass; fused loss avoids b×s×v all-gathers.
- **Empirical breadth:** Strong scaling curves plus accuracy on both decoder-only and encoder-only families, including careful dataset curation (174 GB deduped with LSH).

### Improvements

- **Ablations on alternatives.** Comparisons to other intra-layer sharding layouts (e.g., different splits in MLP or attention output) are limited; only a head-count vs. scaling table appears in Appendix D (Table 7) showing efficiency decreases as heads increase. More head/hidden trade-offs would help.
- **End-task depth.** BERT results emphasize dev-set improvements and RACE test SOTA; wider test-set coverage (e.g., full GLUE test, long-context QA) would clarify generality.
- **Optimizer memory.** The paper flags optimizer memory as a future limitation but doesn’t benchmark optimizer-state sharding/ZeRO-style variants in this work.
- **Data overlap audits.** They report 8-gram overlap checks (e.g., WT103 ≤10.8%, LAMBADA ≤1.4%), which is good practice; publishing scripts and broader leakage analyses would increase confidence. (Figure 6 caption text area on _p.7_.)

### What I Learned / Liked

- The **column-then-row** sharding pattern is the cleanest explanation I’ve seen for minimizing sync points in an MLP block—nice, portable intuition. (Figure 3a on _p.4_.)
- Fusing the **logits + loss** to avoid a gigantic all-gather over vocab is a small code change with an outsized effect.
- For BERT scaling, a **pre-norm/residual ordering tweak** can be the difference between divergence and SOTA—a useful reminder that numerics and layer ordering matter at very large scale.

### Summary Brief

**Megatron-LM** shows that with a handful of well-placed collectives, you can shard Transformer layers **inside** each block (column-parallel MLP input, row-parallel MLP output; attention sharded by heads) so that each layer needs only **two all-reduces per pass**. Embedding/logit sharding plus **loss fusion** slashes communication. On a 32-node DGX-2H cluster (512 V100s), this reaches **15.1 PFLOPs** at **~76%** efficiency and enables training an **8.3B** GPT-2-style LM that sets SOTA on **WikiText-103** and **LAMBADA**, while a **3.9B** BERT-style model attains SOTA on **RACE**. The approach is simple (no compiler), orthogonal to pipeline parallelism, and demonstrates that **intra-layer model parallelism** is a practical path to multi-billion-parameter LMs in mainstream frameworks. According to the **chart on p.2 (Figure 1)** and **scaling plot on p.6 (Figure 5)**, the FLOPs scale smoothly from a single-GPU 39 TF baseline to 512 GPUs with the model+data setup, corroborating the above efficiency claims.
