# Deep Reinforcement Learning from Human Preferences

[arxiv.org/1706.03741](https://arxiv.org/abs/1706.03741)

### Problem / State of the Art

Deep RL typically assumes a well-specified reward, which blocks many real tasks where rewards are hard to program or would incentivize the wrong behavior. This paper asks whether we can _scale_ learning from human preference feedback to modern deep RL, rather than relying on demonstrations or hand-crafted rewards. (See intro and Fig. 1, pp. 1–2.)

### Goal

Learn a reward model from pairwise human comparisons over short video clips, and use it to train high-performing policies—economically (minutes–hours of labeling) and at scale (Atari + MuJoCo). (Abstract & §2, pp. 1, 4–6.)

### Challenges

- Human feedback is scarce/expensive vs. environment interaction.
- Learned rewards can be exploited and are non-stationary as the policy shifts.
- Clips begin from different states, complicating comparisons.
- Label noise and non-expert raters. (Intro & §2, pp. 1–6; ablations §3.3, pp. 9–10.)

### Key Mechanism

- **Preference model:** Fit a neural reward predictor by minimizing cross-entropy under a Bradley-Terry/Luce choice model over _segment_ returns (Eq. 1). Add a fixed annotator-noise term. (p. 5–6.)
- **Policy learning:** Optimize predicted rewards with deep RL (A2C for Atari, TRPO for MuJoCo), normalizing reward scale; train policy and reward model **asynchronously**. (pp. 4–5.)
- **Data flow:** Continuously collect human comparisons of 1–2 s clips; maintain an **ensemble** of reward predictors; optionally query pairs with high ensemble disagreement (active-ish selection). Hold out validation, use L2/dropout. (pp. 5–6.)

### Key Results

- **MuJoCo (Fig. 2, p. 7):** With ~700 labels, performance nearly matches training on the true reward across tasks; with ~1400 labels it sometimes _exceeds_ it (reward shaping from preferences).
- **Atari (Fig. 3, p. 8):** With ~5.5k labels, substantial learning; matches/approaches RL on BeamRider and Pong; slower but improving on Seaquest/Q&#42;bert; below RL on Breakout/SpaceInvaders; human labels outperform RL on Enduro due to shaping.
- **Novel behaviors (§3.2, p. 8):** Learned Hopper backflips (~900 queries, <1 h), Half-Cheetah one-leg gait (~800), and maintaining even pace in Enduro (~1.3k queries, ~4M frames).
- **Ablations (Figs. 5–6, pp. 9–10):** Online querying matters; single frames << clips; offline-trained predictors lead to reward-hacking pathologies (e.g., endless Pong volleys).

### Strengths

- Clear, simple pipeline that _scales_ to deep RL with modest human time.
- Demonstrates behaviors that are hard to hand-specify and even beats true-reward RL in some MuJoCo cases via shaping.
- Solid ablation work identifies what is essential (online updates, segments, comparisons). (pp. 7–10.)

### Improvements

- Replace crude disagreement-based querying with principled **value-of-information** selection; authors note this. (p. 6.)
- Better defenses against reward hacking / non-stationarity (e.g., uncertainty-aware RL objectives, conservatism, periodic human audits).
- Systematic study of segment length and rater guidance; richer feedback types (graded preferences, rationales).
- Generalization tests: transfer across tasks, robustness to adversarial policies, and calibration of the ensemble’s uncertainty.

### What I Learned / Liked

- Pairwise **segment** comparisons plus online training are the sweet spot—humans label faster and the model avoids degenerate optima.
- A learned reward can _shape_ exploration better than the environment’s native reward, sometimes yielding higher returns than “ground truth.” (Figs. 2–3.)

### Summary Brief

The paper trains deep RL agents using a reward model learned from non-expert preferences over short trajectory clips. The reward predictor (Bradley-Terry over segment sums) is trained online in an ensemble while a policy is optimized against its predictions with A2C/TRPO. Across MuJoCo and Atari, a few hundred to a few thousand labels (minutes–hours) are enough to reach near-RL performance, sometimes surpassing it via better reward shaping, and to learn novel tasks like Hopper backflips. Ablations show online preference collection and segment-level comparisons are crucial, while offline learning can induce reward hacking. (pp. 1–10; Figs. 2–6.)
